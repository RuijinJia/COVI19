{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IBj0wvFJJxkd"
   },
   "source": [
    "<b>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"5\">\n",
    "\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"5\">\n",
    "COVID-19 Open Research Dataset Challenge (CORD-19)\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"4\">\n",
    "Authors: Chao Zhou, Ruijin Jia, Matteo Bucalossi\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<font size=\"3\">\n",
    "Machine Learning I (DATS 6202), Spring 2020\n",
    "</font>\n",
    "</center>\n",
    "</p>\n",
    "\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ej6Xp8zJxke"
   },
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SXIN6zbNJxkg"
   },
   "source": [
    "Last month, in response to the global COVID-19 pandemic, the White House and other leading research institutions, including the Allen Institute for AI, have prepared the COVID-19 Open Research Dataset (CORD-19): this dataset includes over 57,000 scholarly articles, most of them with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. As literature published keeps increasing in scale, the medical community would need AI tools to quickly gain insights and directions to fight this disease in a timely manner.  \n",
    "Kaggle has issued a [competition](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge) to develop such tools to help the medical community for this high priority scientific challenge. The public dataset represents a highly readable and clean collection of materials, and it is beeing updated constantly as more publications are released and added to the corpus.  \n",
    "\n",
    "Thanks to the minimal work on pre-processing required by this dataset, we decided to apply a Transformer model to extract sentence embeddings for machine learning task-specific \"heads\". Given that the pre-trained transformer by the UKPLab uses BERT, we decided to train and fine-tune the same transformer on SciBERT by Allen AI to obtain more relevant embeddings for a scientific and medical corpus.  \n",
    "We then used said embeddings to train different clustering algorithms for unsupervised learning: we used UAMP for dimensionality reduction, and HDBSCAN for density-based clustering while LDA for probability-based clustering. Also, we integrated a semantic search algorithm based on cosine similarity that can take user questions (we recommend to take ideas from [Tasks](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks)) and provides the top 5 relevant articles from the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QLsl2Q9JJxkh"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u_YGFBc3Jxkh"
   },
   "source": [
    "Luckily the dataset had already been cleaned for the most part, and we only had to create an hashable corpus of text from json files. Once we had the body text and the abstact for each article, and its relevant metadata, in a dataframe, we cleaned the text columns of odd characters using regex.  Here we call the [preprocessing](https://github.com/matteobucalossi50/CORD-19-Challenge/blob/master/scripts/preprocessing.py) script to obtain a clean dataframe to use for our machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fIhNPzEzJxki"
   },
   "outputs": [],
   "source": [
    "### load preprocessing.py\n",
    "### print dataframe.head or something\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "# import scispacy\n",
    "import spacy\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scripts import preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aEwtfKDdAgZ6"
   },
   "outputs": [],
   "source": [
    "# directories and paths\n",
    "root_path = '/Users/Matteo/Desktop/ML1/project/data/'\n",
    "metadata_path = f'{root_path}/all_sources_metadata_2020-03-13.csv'\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "metadata.head()\n",
    "metadata.info()\n",
    "# read all paths of json file(paper)\n",
    "all_json = glob.glob(f'{root_path}/**/*.json', recursive=True) \n",
    "print(len(all_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 57366 entries, 0 to 57365\n",
      "Data columns (total 18 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   cord_uid                     57366 non-null  object \n",
      " 1   sha                          43540 non-null  object \n",
      " 2   source_x                     57366 non-null  object \n",
      " 3   title                        57203 non-null  object \n",
      " 4   doi                          54020 non-null  object \n",
      " 5   pmcid                        46804 non-null  object \n",
      " 6   pubmed_id                    40905 non-null  float64\n",
      " 7   license                      57366 non-null  object \n",
      " 8   abstract                     46847 non-null  object \n",
      " 9   publish_time                 57358 non-null  object \n",
      " 10  authors                      54840 non-null  object \n",
      " 11  journal                      51576 non-null  object \n",
      " 12  Microsoft Academic Paper ID  964 non-null    float64\n",
      " 13  WHO #Covidence               1768 non-null   object \n",
      " 14  has_pdf_parse                57366 non-null  bool   \n",
      " 15  has_pmc_xml_parse            57366 non-null  bool   \n",
      " 16  full_text_file               48921 non-null  object \n",
      " 17  url                          57060 non-null  object \n",
      "dtypes: bool(2), float64(2), object(14)\n",
      "memory usage: 7.1+ MB\n",
      "68204\n"
     ]
    }
   ],
   "source": [
    "# ######################################testing###################################\n",
    "# # directories and paths\n",
    "# root_path = os.getcwd()\n",
    "# metadata_path = f'{root_path}/metadata.csv'\n",
    "# metadata = pd.read_csv(metadata_path)\n",
    "# metadata.head()\n",
    "# metadata.info()\n",
    "# # read all paths of json file(paper)\n",
    "# all_json = glob.glob(f'{root_path}/**/*.json', recursive=True) \n",
    "# print(len(all_json))\n",
    "# ######################################testing###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var</th>\n",
       "      <th>proportion</th>\n",
       "      <th>dtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Microsoft Academic Paper ID</td>\n",
       "      <td>0.983196</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WHO #Covidence</td>\n",
       "      <td>0.969180</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pubmed_id</td>\n",
       "      <td>0.286947</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sha</td>\n",
       "      <td>0.241014</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pmcid</td>\n",
       "      <td>0.184116</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>abstract</td>\n",
       "      <td>0.183366</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>full_text_file</td>\n",
       "      <td>0.147213</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>journal</td>\n",
       "      <td>0.100931</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>doi</td>\n",
       "      <td>0.058327</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>authors</td>\n",
       "      <td>0.044033</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>url</td>\n",
       "      <td>0.005334</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>title</td>\n",
       "      <td>0.002841</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>publish_time</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            var  proportion    dtype\n",
       "0   Microsoft Academic Paper ID    0.983196  float64\n",
       "1                WHO #Covidence    0.969180   object\n",
       "2                     pubmed_id    0.286947  float64\n",
       "3                           sha    0.241014   object\n",
       "4                         pmcid    0.184116   object\n",
       "5                      abstract    0.183366   object\n",
       "6                full_text_file    0.147213   object\n",
       "7                       journal    0.100931   object\n",
       "8                           doi    0.058327   object\n",
       "9                       authors    0.044033   object\n",
       "10                          url    0.005334   object\n",
       "11                        title    0.002841   object\n",
       "12                 publish_time    0.000139   object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nan_checker(df):\n",
    "    \n",
    "    # Get the variables with NaN, their proportion of NaN and dtype\n",
    "    df_nan = pd.DataFrame([[var, df[var].isna().sum() / df.shape[0], df[var].dtype]\n",
    "                           for var in df.columns if df[var].isna().sum() > 0],\n",
    "                          columns=['var', 'proportion', 'dtype'])\n",
    "    \n",
    "    # Sort df_nan in accending order of the proportion of NaN\n",
    "    df_nan = df_nan.sort_values(by='proportion', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return df_nan\n",
    "nan_checker(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46847, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sha</th>\n",
       "      <th>source_x</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>Microsoft Academic Paper ID</th>\n",
       "      <th>WHO #Covidence</th>\n",
       "      <th>has_pdf_parse</th>\n",
       "      <th>has_pmc_xml_parse</th>\n",
       "      <th>full_text_file</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zjufx4fo</td>\n",
       "      <td>b2897e1277f56641193a6db73825f707eed3e4c9</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Sequence requirements for RNA strand transfer ...</td>\n",
       "      <td>10.1093/emboj/20.24.7220</td>\n",
       "      <td>PMC125340</td>\n",
       "      <td>11742998.0</td>\n",
       "      <td>unk</td>\n",
       "      <td>Nidovirus subgenomic mRNAs contain a leader se...</td>\n",
       "      <td>2001-12-17</td>\n",
       "      <td>Pasternak, Alexander O.; van den Born, Erwin; ...</td>\n",
       "      <td>The EMBO Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>http://europepmc.org/articles/pmc125340?pdf=re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ymceytj3</td>\n",
       "      <td>e3d0d482ebd9a8ba81c254cc433f314142e72174</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Crystal structure of murine sCEACAM1a[1,4]: a ...</td>\n",
       "      <td>10.1093/emboj/21.9.2076</td>\n",
       "      <td>PMC125375</td>\n",
       "      <td>11980704.0</td>\n",
       "      <td>unk</td>\n",
       "      <td>CEACAM1 is a member of the carcinoembryonic an...</td>\n",
       "      <td>2002-05-01</td>\n",
       "      <td>Tan, Kemin; Zelus, Bruce D.; Meijers, Rob; Liu...</td>\n",
       "      <td>The EMBO Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>http://europepmc.org/articles/pmc125375?pdf=re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wzj2glte</td>\n",
       "      <td>00b1d99e70f779eb4ede50059db469c65e8c1469</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Synthesis of a novel hepatitis C virus protein...</td>\n",
       "      <td>10.1093/emboj/20.14.3840</td>\n",
       "      <td>PMC125543</td>\n",
       "      <td>11447125.0</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>Hepatitis C virus (HCV) is an important human ...</td>\n",
       "      <td>2001-07-16</td>\n",
       "      <td>Xu, Zhenming; Choi, Jinah; Yen, T.S.Benedict; ...</td>\n",
       "      <td>EMBO J</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2sfqsfm1</td>\n",
       "      <td>cf584e00f637cbd8f1bb35f3f09f5ed07b71aeb0</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Structure of coronavirus main proteinase revea...</td>\n",
       "      <td>10.1093/emboj/cdf327</td>\n",
       "      <td>PMC126080</td>\n",
       "      <td>12093723.0</td>\n",
       "      <td>unk</td>\n",
       "      <td>The key enzyme in coronavirus polyprotein proc...</td>\n",
       "      <td>2002-07-01</td>\n",
       "      <td>Anand, Kanchan; Palm, Gottfried J.; Mesters, J...</td>\n",
       "      <td>The EMBO Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>http://europepmc.org/articles/pmc126080?pdf=re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i0zym7iq</td>\n",
       "      <td>dde02f11923815e6a16a31dd6298c46b109c5dfa</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Discontinuous and non-discontinuous subgenomic...</td>\n",
       "      <td>10.1093/emboj/cdf635</td>\n",
       "      <td>PMC136939</td>\n",
       "      <td>12456663.0</td>\n",
       "      <td>unk</td>\n",
       "      <td>Arteri-, corona-, toro- and roniviruses are ev...</td>\n",
       "      <td>2002-12-01</td>\n",
       "      <td>van Vliet, A.L.W.; Smits, S.L.; Rottier, P.J.M...</td>\n",
       "      <td>The EMBO Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>http://europepmc.org/articles/pmc136939?pdf=re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cord_uid                                       sha source_x  \\\n",
       "0  zjufx4fo  b2897e1277f56641193a6db73825f707eed3e4c9      PMC   \n",
       "1  ymceytj3  e3d0d482ebd9a8ba81c254cc433f314142e72174      PMC   \n",
       "2  wzj2glte  00b1d99e70f779eb4ede50059db469c65e8c1469      PMC   \n",
       "3  2sfqsfm1  cf584e00f637cbd8f1bb35f3f09f5ed07b71aeb0      PMC   \n",
       "4  i0zym7iq  dde02f11923815e6a16a31dd6298c46b109c5dfa      PMC   \n",
       "\n",
       "                                               title  \\\n",
       "0  Sequence requirements for RNA strand transfer ...   \n",
       "1  Crystal structure of murine sCEACAM1a[1,4]: a ...   \n",
       "2  Synthesis of a novel hepatitis C virus protein...   \n",
       "3  Structure of coronavirus main proteinase revea...   \n",
       "4  Discontinuous and non-discontinuous subgenomic...   \n",
       "\n",
       "                        doi      pmcid   pubmed_id license  \\\n",
       "0  10.1093/emboj/20.24.7220  PMC125340  11742998.0     unk   \n",
       "1   10.1093/emboj/21.9.2076  PMC125375  11980704.0     unk   \n",
       "2  10.1093/emboj/20.14.3840  PMC125543  11447125.0   no-cc   \n",
       "3      10.1093/emboj/cdf327  PMC126080  12093723.0     unk   \n",
       "4      10.1093/emboj/cdf635  PMC136939  12456663.0     unk   \n",
       "\n",
       "                                            abstract publish_time  \\\n",
       "0  Nidovirus subgenomic mRNAs contain a leader se...   2001-12-17   \n",
       "1  CEACAM1 is a member of the carcinoembryonic an...   2002-05-01   \n",
       "2  Hepatitis C virus (HCV) is an important human ...   2001-07-16   \n",
       "3  The key enzyme in coronavirus polyprotein proc...   2002-07-01   \n",
       "4  Arteri-, corona-, toro- and roniviruses are ev...   2002-12-01   \n",
       "\n",
       "                                             authors           journal  \\\n",
       "0  Pasternak, Alexander O.; van den Born, Erwin; ...  The EMBO Journal   \n",
       "1  Tan, Kemin; Zelus, Bruce D.; Meijers, Rob; Liu...  The EMBO Journal   \n",
       "2  Xu, Zhenming; Choi, Jinah; Yen, T.S.Benedict; ...            EMBO J   \n",
       "3  Anand, Kanchan; Palm, Gottfried J.; Mesters, J...  The EMBO Journal   \n",
       "4  van Vliet, A.L.W.; Smits, S.L.; Rottier, P.J.M...  The EMBO Journal   \n",
       "\n",
       "   Microsoft Academic Paper ID WHO #Covidence  has_pdf_parse  \\\n",
       "0                          NaN            NaN           True   \n",
       "1                          NaN            NaN           True   \n",
       "2                          NaN            NaN           True   \n",
       "3                          NaN            NaN           True   \n",
       "4                          NaN            NaN           True   \n",
       "\n",
       "   has_pmc_xml_parse  full_text_file  \\\n",
       "0               True  custom_license   \n",
       "1               True  custom_license   \n",
       "2               True  custom_license   \n",
       "3               True  custom_license   \n",
       "4               True  custom_license   \n",
       "\n",
       "                                                 url  \n",
       "0  http://europepmc.org/articles/pmc125340?pdf=re...  \n",
       "1  http://europepmc.org/articles/pmc125375?pdf=re...  \n",
       "2  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...  \n",
       "3  http://europepmc.org/articles/pmc126080?pdf=re...  \n",
       "4  http://europepmc.org/articles/pmc136939?pdf=re...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop the rows where abstract is missing\n",
    "metadata.dropna(subset = ['abstract'],axis = 0, inplace = True)\n",
    "#check dimension of new dataset\n",
    "print(metadata.shape)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45989, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sha</th>\n",
       "      <th>source_x</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>Microsoft Academic Paper ID</th>\n",
       "      <th>WHO #Covidence</th>\n",
       "      <th>has_pdf_parse</th>\n",
       "      <th>has_pmc_xml_parse</th>\n",
       "      <th>full_text_file</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zjufx4fo</td>\n",
       "      <td>b2897e1277f56641193a6db73825f707eed3e4c9</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Sequence requirements for RNA strand transfer ...</td>\n",
       "      <td>10.1093/emboj/20.24.7220</td>\n",
       "      <td>PMC125340</td>\n",
       "      <td>11742998.0</td>\n",
       "      <td>unk</td>\n",
       "      <td>Nidovirus subgenomic mRNAs contain a leader se...</td>\n",
       "      <td>2001-12-17</td>\n",
       "      <td>Pasternak, Alexander O.; van den Born, Erwin; ...</td>\n",
       "      <td>The EMBO Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>http://europepmc.org/articles/pmc125340?pdf=re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ymceytj3</td>\n",
       "      <td>e3d0d482ebd9a8ba81c254cc433f314142e72174</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Crystal structure of murine sCEACAM1a[1,4]: a ...</td>\n",
       "      <td>10.1093/emboj/21.9.2076</td>\n",
       "      <td>PMC125375</td>\n",
       "      <td>11980704.0</td>\n",
       "      <td>unk</td>\n",
       "      <td>CEACAM1 is a member of the carcinoembryonic an...</td>\n",
       "      <td>2002-05-01</td>\n",
       "      <td>Tan, Kemin; Zelus, Bruce D.; Meijers, Rob; Liu...</td>\n",
       "      <td>The EMBO Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>http://europepmc.org/articles/pmc125375?pdf=re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wzj2glte</td>\n",
       "      <td>00b1d99e70f779eb4ede50059db469c65e8c1469</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Synthesis of a novel hepatitis C virus protein...</td>\n",
       "      <td>10.1093/emboj/20.14.3840</td>\n",
       "      <td>PMC125543</td>\n",
       "      <td>11447125.0</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>Hepatitis C virus (HCV) is an important human ...</td>\n",
       "      <td>2001-07-16</td>\n",
       "      <td>Xu, Zhenming; Choi, Jinah; Yen, T.S.Benedict; ...</td>\n",
       "      <td>EMBO J</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2sfqsfm1</td>\n",
       "      <td>cf584e00f637cbd8f1bb35f3f09f5ed07b71aeb0</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Structure of coronavirus main proteinase revea...</td>\n",
       "      <td>10.1093/emboj/cdf327</td>\n",
       "      <td>PMC126080</td>\n",
       "      <td>12093723.0</td>\n",
       "      <td>unk</td>\n",
       "      <td>The key enzyme in coronavirus polyprotein proc...</td>\n",
       "      <td>2002-07-01</td>\n",
       "      <td>Anand, Kanchan; Palm, Gottfried J.; Mesters, J...</td>\n",
       "      <td>The EMBO Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>http://europepmc.org/articles/pmc126080?pdf=re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i0zym7iq</td>\n",
       "      <td>dde02f11923815e6a16a31dd6298c46b109c5dfa</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Discontinuous and non-discontinuous subgenomic...</td>\n",
       "      <td>10.1093/emboj/cdf635</td>\n",
       "      <td>PMC136939</td>\n",
       "      <td>12456663.0</td>\n",
       "      <td>unk</td>\n",
       "      <td>Arteri-, corona-, toro- and roniviruses are ev...</td>\n",
       "      <td>2002-12-01</td>\n",
       "      <td>van Vliet, A.L.W.; Smits, S.L.; Rottier, P.J.M...</td>\n",
       "      <td>The EMBO Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>http://europepmc.org/articles/pmc136939?pdf=re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cord_uid                                       sha source_x  \\\n",
       "0  zjufx4fo  b2897e1277f56641193a6db73825f707eed3e4c9      PMC   \n",
       "1  ymceytj3  e3d0d482ebd9a8ba81c254cc433f314142e72174      PMC   \n",
       "2  wzj2glte  00b1d99e70f779eb4ede50059db469c65e8c1469      PMC   \n",
       "3  2sfqsfm1  cf584e00f637cbd8f1bb35f3f09f5ed07b71aeb0      PMC   \n",
       "4  i0zym7iq  dde02f11923815e6a16a31dd6298c46b109c5dfa      PMC   \n",
       "\n",
       "                                               title  \\\n",
       "0  Sequence requirements for RNA strand transfer ...   \n",
       "1  Crystal structure of murine sCEACAM1a[1,4]: a ...   \n",
       "2  Synthesis of a novel hepatitis C virus protein...   \n",
       "3  Structure of coronavirus main proteinase revea...   \n",
       "4  Discontinuous and non-discontinuous subgenomic...   \n",
       "\n",
       "                        doi      pmcid   pubmed_id license  \\\n",
       "0  10.1093/emboj/20.24.7220  PMC125340  11742998.0     unk   \n",
       "1   10.1093/emboj/21.9.2076  PMC125375  11980704.0     unk   \n",
       "2  10.1093/emboj/20.14.3840  PMC125543  11447125.0   no-cc   \n",
       "3      10.1093/emboj/cdf327  PMC126080  12093723.0     unk   \n",
       "4      10.1093/emboj/cdf635  PMC136939  12456663.0     unk   \n",
       "\n",
       "                                            abstract publish_time  \\\n",
       "0  Nidovirus subgenomic mRNAs contain a leader se...   2001-12-17   \n",
       "1  CEACAM1 is a member of the carcinoembryonic an...   2002-05-01   \n",
       "2  Hepatitis C virus (HCV) is an important human ...   2001-07-16   \n",
       "3  The key enzyme in coronavirus polyprotein proc...   2002-07-01   \n",
       "4  Arteri-, corona-, toro- and roniviruses are ev...   2002-12-01   \n",
       "\n",
       "                                             authors           journal  \\\n",
       "0  Pasternak, Alexander O.; van den Born, Erwin; ...  The EMBO Journal   \n",
       "1  Tan, Kemin; Zelus, Bruce D.; Meijers, Rob; Liu...  The EMBO Journal   \n",
       "2  Xu, Zhenming; Choi, Jinah; Yen, T.S.Benedict; ...            EMBO J   \n",
       "3  Anand, Kanchan; Palm, Gottfried J.; Mesters, J...  The EMBO Journal   \n",
       "4  van Vliet, A.L.W.; Smits, S.L.; Rottier, P.J.M...  The EMBO Journal   \n",
       "\n",
       "   Microsoft Academic Paper ID WHO #Covidence  has_pdf_parse  \\\n",
       "0                          NaN            NaN           True   \n",
       "1                          NaN            NaN           True   \n",
       "2                          NaN            NaN           True   \n",
       "3                          NaN            NaN           True   \n",
       "4                          NaN            NaN           True   \n",
       "\n",
       "   has_pmc_xml_parse  full_text_file  \\\n",
       "0               True  custom_license   \n",
       "1               True  custom_license   \n",
       "2               True  custom_license   \n",
       "3               True  custom_license   \n",
       "4               True  custom_license   \n",
       "\n",
       "                                                 url  \n",
       "0  http://europepmc.org/articles/pmc125340?pdf=re...  \n",
       "1  http://europepmc.org/articles/pmc125375?pdf=re...  \n",
       "2  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...  \n",
       "3  http://europepmc.org/articles/pmc126080?pdf=re...  \n",
       "4  http://europepmc.org/articles/pmc136939?pdf=re...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove dupicate with same title\n",
    "metadata.drop_duplicates(subset =\"title\", keep = False, inplace = True)\n",
    "print(metadata.shape)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O8EhNEaBAgZ9"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'abstract'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-8274b24f1bf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# read json files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfirst_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_row\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Spring 2020/ML1/final_repo/CORD-19-Challenge/scripts/preprocessing.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;31m# Abstract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'abstract'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;31m# Body text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'abstract'"
     ]
    }
   ],
   "source": [
    "# read json files\n",
    "first_row = preprocessing.FileReader(all_json[0])\n",
    "print(first_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-aaHcRxeAgaA"
   },
   "outputs": [],
   "source": [
    "# build dataframe\n",
    "df_covid = preprocessing.read_directory_files(all_json)\n",
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZaPDe4JcAgaC"
   },
   "outputs": [],
   "source": [
    "# clean abstract and body_text\n",
    "cleaned_abstract = []\n",
    "for item in df_covid['abstract']:\n",
    "    item = preprocessing.clean_text(item)\n",
    "    cleaned_abstract.append(item)\n",
    "df_covid['abstract'] = cleaned_abstract\n",
    "\n",
    "#clean body_text\n",
    "cleaned_body = []\n",
    "for item in df_covid['body_text']:\n",
    "    item = preprocessing.clean_text(item)\n",
    "    cleaned_body.append(item)\n",
    "df_covid['body_text'] = cleaned_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q6lVwDHUAgaE"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame([[df_covid.shape[0], df_covid.shape[1]]], columns=['# rows', '# columns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fSlSOYxGAgaI"
   },
   "outputs": [],
   "source": [
    "# save dataframe\n",
    "df_covid.to_pickle('/Users/Matteo/Desktop/ML1/project/data/preprocessed_dataframe.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6yhK5cPFJxkm"
   },
   "source": [
    "# Train Sentence Transformer on sciBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SKziMCBmJxkn"
   },
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x3HdNSjzJxko"
   },
   "source": [
    "When it comes to natural language processing, the most recent developments have seen attention mechanisms prevailing versus more traditiona RNN models. These models use an architecture called Transformer, much faster and easier to parallelize than other networks. This is where these models revolutionize the field: an attention mechanism looks at an input sequence and decides at each step which other components of the sequence are important<sup>[1](https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04)</sup> - meaning it can replicate the way we actually process text, i.e. not only focusing on single words but also considering what's around it to make sense of the language.   \n",
    "\n",
    "Transformers are architectures for transforming a sequence into another by using Encoder and Decoder; yet, they only imply attention mechanisms without any Recurrent Networks (previously the go-to models for many NLP tasks). Here's an image<sup>[2](https://arxiv.org/abs/1706.03762)</sup>  to illustrate such Transformer architecture, with the Encoder part on the left and the Decoder on the right.\n",
    "![](images/Transformers_scheme.png)\n",
    "We won't bore you with the details of the model and its mathematical aspects, but we can point out two main characteristics of Transformers:\n",
    " - the Multi-Head Attention layers treat each word's relationship with every other word in the same sentence, basically paying attention to more words than just one when processing sequences. These layers will apply to every (input/target, depending on encoder/decoder) sentence the following equation: ![](images/scaled dot-prod attention.png)\n",
    " - the positional encodings of words are dense vectors (some extra word embeddings) representing the position of a word within the sentencem and are added to each word's embeddings.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CWUH-KBOJxkp"
   },
   "source": [
    "### BERT & SciBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gFs06yP_Jxkq"
   },
   "source": [
    "A 2018 paper<sup>[3](https://arxiv.org/abs/1810.04805)</sup> published by various Google researchers brought to life a nowadays state-of-the-art application of a Transformer-based architectures for self-supervised pretraining on large corpus, BERT (*Bidirectional Encoder Representations from Transformers*).  BERT is a method of pre-training language representations, so that we can train a general-purpose model on an immense corpus and then use said model for downstream NLP tasks - its bidirectional characteristic allows to represent each word within its context (i.e. other words in the sentence, both on the left and right of represented word).\n",
    "\n",
    "If the original BERT trained a large (12-layer to 24-layer) Transformer on a large corpuse of Wikipedia and BookCorpus, more recent BERT-alike models have trained the same architecture on specific corpuses for domain-relevant tasks. For instance, in 2019 Allen AI released SciBERT,<sup>[4](https://arxiv.org/abs/1903.10676)</sup> a BERT model trained on huge corpus of scientific papers (82% of which from biomedical domain)  from [semanticscholar.org](https://www.semanticscholar.org/), which significantly improves BERT performance on downstream NLP tasks specific to scientific problems. We believe that using SciBERT for this project will yield much better results than the generic original BERT model given the specificity of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sr16_xEtJxkr"
   },
   "source": [
    "### Sentence-BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xj5xGOstJxks"
   },
   "source": [
    "BERT-alike models described above have set a new bar for sentence-pair regressions tasks, but unfortunately they need both sentences to be fed in the Transformer, causing such a computational overhead that makes unsupervised tasks virtually impossible.  \n",
    "Thus, in 2019 researchers at UKPLab released SBERT,<sup>[5](https://arxiv.org/abs/1908.10084)</sup> a modification of pre-trained BERT to derive semantic sentence embeddings easily comparable for similarity and clustering. SBERT fine-tunes BERT-alike models with a siamese or triplet network structure to obtain such embeddings. Such a revolutionary paper proposed a model that still maintains BERT-level accuracy, while scaling down time complexity from 65 hours to 5 seconds for specific unsupervised NLP tasks, including similarity comparison, semantic search and clustering. (ah! exactly what we are trying to do here!)  \n",
    "Here's an illustration of the SBERT architecture as described in the paper (the structure would not change if the objective function was different, as it may be for different tasks): ![](images/SBERT architecture.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6it8mgZwJxkt"
   },
   "source": [
    "# Sentence embeddings for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rOuKv-xOJxku"
   },
   "source": [
    "Now, we intend to perform some clustering as well as semantic search on the CORD-19 dataset. To do so, we need SBERT sentence embeddings so that we can accomplish these tasks in reasonable time. We decided that pre-trained embeddings on BERT would have not provide the SOTA outcome we were looking for, thus we fine-tuned our sentence embedding method on SciBERT to get science-specific sentence embeddings.  \n",
    "The SBERT library provides the code to tune any BERT-like model (in our case, SciBERT) on the Natural Language Inference (NLI) data, published by Allen AI. The following [script](https://github.com/matteobucalossi50/CORD-19-Challenge/blob/master/scripts/training_scibert.py) trained SciBERT on the NLI dataset using a Softmax Classifier as training loss and the STS (Semantic Text Similarity) dataset as benchmark for evaluation, and provided us a file-tuned sentence embedder to derive embeddings for our clustering and search tasks on the CORD-19 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EFnZFLoMAgaQ"
   },
   "source": [
    "```python\n",
    "# select one Transformer\n",
    "model_name = 'allenai/scibert_scivocab_uncased'\n",
    "\n",
    "# Use sciBERT model for mapping tokens to embeddings\n",
    "word_embedding_model = models.BERT(model_name)\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode_mean_tokens=True,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False)\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "# Convert the dataset to a DataLoader ready for training\n",
    "train_data = SentencesDataset(nli_reader.get_examples('train.gz'), model=model)\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "train_loss = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=train_num_labels)\n",
    "\n",
    "dev_data = SentencesDataset(examples=sts_reader.get_examples('sts-dev.csv'), model=model)\n",
    "dev_dataloader = DataLoader(dev_data, shuffle=False, batch_size=batch_size)\n",
    "evaluator = EmbeddingSimilarityEvaluator(dev_dataloader)\n",
    "\n",
    "num_epochs = 2\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs / batch_size * 0.1) #10% of train data for warm-up\n",
    "# Train the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path\n",
    "          )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YJzAc_otJxku"
   },
   "outputs": [],
   "source": [
    "The training of the sentence transformer took eventually around 5 hours when performed on Kaggle notebook with GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5j3hkLQKJxkz"
   },
   "source": [
    "At this point, we can simply use our fine-tuned model (perfect for our biomedical dataset) to encode our corpus. This script will provide the embeddings we need, both for the abstract and separately for the full text of each article. We will then access these embeddings in the dataframe for downstream tasks below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C5t_5xNLJxk0"
   },
   "outputs": [],
   "source": [
    "## import embeddings.py\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import embeddings\n",
    "\n",
    "# download pre-trained model\n",
    "model = SentenceTransformer('training_nli_allenai-scibert_scivocab_uncased-2020-04-26_13-22-06') #this or the model we trained and saved\n",
    "\n",
    "# import dataframe without embeedings yet\n",
    "df_covid = pd.read_pickle('/Users/Matteo/Desktop/ML1/project/data/preprocessed_dataframe.pkl')  # hopefully this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1IVLOr9KAgaW"
   },
   "outputs": [],
   "source": [
    "# add abstract embeddings to dataframe\n",
    "df_covid['abs_embeddings'] = embeddings.sent_embeddings(df_covid['abstract'], model)\n",
    "\n",
    "# add full text embeddings to dataframe\n",
    "df_covid['body_embeddings'] = embeddings.sent_embeddings(df_covid['body_text'], model)\n",
    "\n",
    "# save dataframe\n",
    "df_covid.to_pickle('./data/preprocessed_dataframe.pkl')\n",
    "\n",
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M_EffQU0Jxk4"
   },
   "source": [
    "# Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ai2NaZnVRw6g"
   },
   "source": [
    "Now we are going to reduce the dimensions and do clustering. UMAP is applied to reduce dimension keeping 95% variance, which will retain meaningful information and removes noisies to make clustering easier. Then HDBSCAN and K-Means are applied to cluster and labelled group. Finally we are going to modeling topic in each clustering group by LDA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ioonc-5RQ8zF"
   },
   "source": [
    "## UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N3XW50wtPfSO"
   },
   "source": [
    "Uniform Manifold Approximation and Projection (UMAP)<sup>[6](https://arxiv.org/abs/1802.03426)</sup> is an algorithm for dimension reduction based on manifold learning techniques and ideas from topological data analysis. It provides a very general framework for approaching manifold learning and dimension reduction, but can also provide specific concrete realizations and can preserve more of the global structure with superior run time performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BSxszVvcWcjT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: umap in /usr/local/lib/python3.7/site-packages (0.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fONaz83-Ar6p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting umap-learn\n",
      "  Downloading umap-learn-0.4.1.tar.gz (68 kB)\n",
      "\u001b[K     |████████████████████████████████| 68 kB 378 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/site-packages (from umap-learn) (1.18.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/site-packages (from umap-learn) (0.22.2.post1)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/site-packages (from umap-learn) (1.4.1)\n",
      "Collecting numba!=0.47,>=0.46\n",
      "  Downloading numba-0.49.0-cp37-cp37m-macosx_10_14_x86_64.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 302 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tbb>=2019.0\n",
      "  Downloading tbb-2019.0-py2.py3-none-macosx_10_12_intel.macosx_10_12_x86_64.whl (565 kB)\n",
      "\u001b[K     |████████████████████████████████| 565 kB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/site-packages (from scikit-learn>=0.20->umap-learn) (0.14.0)\n",
      "Collecting llvmlite<=0.33.0.dev0,>=0.31.0.dev0\n",
      "  Downloading llvmlite-0.32.0-cp37-cp37m-macosx_10_9_x86_64.whl (15.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.9 MB 9.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from numba!=0.47,>=0.46->umap-learn) (45.2.0)\n",
      "Building wheels for collected packages: umap-learn\n",
      "  Building wheel for umap-learn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for umap-learn: filename=umap_learn-0.4.1-py3-none-any.whl size=66312 sha256=4d7225f036a7bce75a296e46ee0c4eb542e99e51f2f7757091ad9f42d01faf68\n",
      "  Stored in directory: /Users/chao/Library/Caches/pip/wheels/3b/73/50/6fe0ae0386b5803013b304a139e3d48bbea84192f6ae82f615\n",
      "Successfully built umap-learn\n",
      "Installing collected packages: llvmlite, numba, tbb, umap-learn\n",
      "Successfully installed llvmlite-0.32.0 numba-0.49.0 tbb-2019.0 umap-learn-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nvoFPFbpAvvL"
   },
   "outputs": [],
   "source": [
    "import umap.umap_ as umap\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wi8Fa2MIBN8v"
   },
   "outputs": [],
   "source": [
    "#open pickle file to extract vextorization\n",
    "# df=open('/content/drive/My Drive/Colab Notebooks/final/preprocessed_dataframe_withabs.pkl','rb')\n",
    "df=pd.read_pickle('compelete_dataframe.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GrnJKUg8AyXy"
   },
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_neighbors = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JCTsyBHWA3Od"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-56adfd44c497>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnumpy_array\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'abs_embeddings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'abs_embeddings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mnumpy_array\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrow_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'abs_embeddings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#for abstract\n",
    "numpy_array=df['abs_embeddings'][0]\n",
    "for i in range(1,len(df['abs_embeddings'])):\n",
    "  numpy_array=np.row_stack((numpy_array,df['abs_embeddings'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hwArQ4NWMC19"
   },
   "outputs": [],
   "source": [
    "#for body text\n",
    "numpy_array2=df['body_embeddings'][0]\n",
    "for i in range(1,len(df['body_embeddings'])):\n",
    "  numpy_array2=np.row_stack((numpy_array2,df['body_embeddings'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AkxVWsZNCVBW"
   },
   "source": [
    "###Visulization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uRFcLy-IMZQV"
   },
   "source": [
    "#####Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4elmY6cA73h"
   },
   "outputs": [],
   "source": [
    "# reduce to two dimentions and plot result\n",
    "clusterable_embedding = reducer.fit_transform(np.asmatrix(numpy_array))\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(clusterable_embedding[:,0],clusterable_embedding[:,1])\n",
    "clusterable_embedding.shape\n",
    "print(clusterable_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q_V7RZEMMcLD"
   },
   "source": [
    "#####Body Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "quRd8x5eMVHI"
   },
   "outputs": [],
   "source": [
    "clusterable_embedding2 = reducer.fit_transform(np.asmatrix(numpy_array2))\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(clusterable_embedding2[:,0],clusterable_embedding2[:,1])\n",
    "clusterable_embedding2.shape\n",
    "print(clusterable_embedding2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aTYLWxxGJxk4"
   },
   "source": [
    "## HDBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7FganRuEWnw6"
   },
   "source": [
    "We are trying to run vectorization and separate the literature by two ways, the first one is HDBSCAN. HDBSCAN is a clustering algorithm developed by Campello, Moulavi, and Sander.<sup>[7](https://link.springer.com/chapter/10.1007%2F978-3-642-37456-2_14)</sup> It extends DBSCAN (a classic density-based spatial algorithm) by converting it into a hierarchical clustering algorithm, and then using a technique to extract a flat clustering based in the stability of clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jJKpoFwAYG9U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Password:\n"
     ]
    }
   ],
   "source": [
    "!sudo -H pip3 install hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lnjNitoACJBB"
   },
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U0IXfZxzCS83"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clusterable_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-552d1c359905>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Abstract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclusterer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhdbscan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHDBSCAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_cluster_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_min_span_tree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclusterer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclusterer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclusterable_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'clusterable_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "#Abstract\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=2, gen_min_span_tree=True)\n",
    "clusterer=clusterer.fit(clusterable_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "chQ3QLLwMsCL"
   },
   "outputs": [],
   "source": [
    "#Body Text\n",
    "clusterer2=clusterer.fit(clusterable_embedding2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4TMk0L7yJxk5"
   },
   "source": [
    "### visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L_AagW6BCjaN"
   },
   "source": [
    "#### Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d4unvRRtWAIk"
   },
   "outputs": [],
   "source": [
    "#Build the minimum spanning tree\n",
    "clusterer.minimum_spanning_tree_.plot(edge_cmap='viridis',\n",
    "                                      edge_alpha=0.6,\n",
    "                                      node_size=80,\n",
    "                                      edge_linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Exbn_wawCweH"
   },
   "outputs": [],
   "source": [
    "#Build the cluster hierarchy\n",
    "clusterer.single_linkage_tree_.plot(cmap='viridis', colorbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zlVojO4DCzAE"
   },
   "outputs": [],
   "source": [
    "#Condense the cluster tree\n",
    "clusterer.condensed_tree_.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-xpDgPQdC15v"
   },
   "outputs": [],
   "source": [
    "#Extract the clusters\n",
    "clusterer.condensed_tree_.plot(select_clusters=True, selection_palette=sns.color_palette())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XQ7xdTjxC45A"
   },
   "outputs": [],
   "source": [
    "#clusterer = hdbscan.HDBSCAN(min_cluster_size=10, prediction_data=True).fit(clusterable_embedding)\n",
    "color_palette = sns.color_palette('Paired',max(clusterer.labels_))\n",
    "cluster_colors = [color_palette[x] if x >= 0 and x<max(clusterer.labels_)\n",
    "                  else (0.5, 0.5, 0.5)\n",
    "                  for x in clusterer.labels_]\n",
    "cluster_member_colors = [sns.desaturate(x, p) for x, p in\n",
    "                        zip(cluster_colors, clusterer.probabilities_)]\n",
    "plt.scatter(*clusterable_embedding.T, s=50, linewidth=0, c=cluster_member_colors, alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XRiX7DtAC7eg"
   },
   "source": [
    "we got more than 1000 clustering groups totally, so HDBSCAN may not be the best way to cluster, and then we will try K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-c7oKIndL3O1"
   },
   "source": [
    "####Body Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KTxyqHYvMkMM"
   },
   "outputs": [],
   "source": [
    "#Build the minimum spanning tree\n",
    "clusterer2.minimum_spanning_tree_.plot(edge_cmap='viridis',\n",
    "                                      edge_alpha=0.6,\n",
    "                                      node_size=80,\n",
    "                                      edge_linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d3_3oIkINNZ0"
   },
   "outputs": [],
   "source": [
    "#Build the cluster hierarchy\n",
    "clusterer2.single_linkage_tree_.plot(cmap='viridis', colorbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4t0BZ_ffNRI0"
   },
   "outputs": [],
   "source": [
    "#Condense the cluster tree\n",
    "clusterer2.condensed_tree_.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vn8pLF8cNUJM"
   },
   "outputs": [],
   "source": [
    "#Extract the clusters\n",
    "clusterer2.condensed_tree_.plot(select_clusters=True, selection_palette=sns.color_palette())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ppXhRlM3NW2d"
   },
   "outputs": [],
   "source": [
    "#clusterer = hdbscan.HDBSCAN(min_cluster_size=10, prediction_data=True).fit(clusterable_embedding)\n",
    "color_palette = sns.color_palette('Paired',max(clusterer2.labels_))\n",
    "cluster_colors = [color_palette[x] if x >= 0 and x<max(clusterer2.labels_)\n",
    "                  else (0.5, 0.5, 0.5)\n",
    "                  for x in clusterer.labels_]\n",
    "cluster_member_colors2 = [sns.desaturate(x, p) for x, p in\n",
    "                        zip(cluster_colors, clusterer2.probabilities_)]\n",
    "plt.scatter(*clusterable_embedding2.T, s=50, linewidth=0, c=cluster_member_colors2, alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oNpFVu8fJxk5"
   },
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U7evGbOMYt9b"
   },
   "source": [
    "Then we will see what k-means clustering to be like and what makes it different from HDBSCAN. First step is finding best k-value. Distortion computes the sum of squared distances from each point to its assigned center. When distortion is plotted against k there will be a k value after which decreases in distortion are minimal. This is the desired number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pAvh3XTbZQIK"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vyqd8jSBNuVs"
   },
   "source": [
    "####Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hkmyfljVJ7Lj"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clusterable_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c40688e629a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mk_means\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclusterable_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mk_means\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclusterable_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdistortions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclusterable_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_means\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'euclidean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clusterable_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "distortions = []\n",
    "K = range(2, 40)\n",
    "for k in K:\n",
    "    k_means = KMeans(n_clusters=k, random_state=42).fit(clusterable_embedding)\n",
    "    k_means.fit(clusterable_embedding)\n",
    "    distortions.append(sum(np.min(cdist(clusterable_embedding, k_means.cluster_centers_, 'euclidean'), axis=1)) / pd.DataFrame(data3).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KYpSvGqtKAyc"
   },
   "outputs": [],
   "source": [
    "X_line = [K[0], K[-1]]\n",
    "Y_line = [distortions[0], distortions[-1]]\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'b-')\n",
    "plt.plot(X_line, Y_line, 'r')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJyg8KshKDrL"
   },
   "source": [
    "The best k-value is about 9 to 12, so we determined the best one is 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CPhhNRRxDPYw"
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10, random_state=0).fit_predict(clusterable_embedding)\n",
    "kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q8VBTgPgDTlw"
   },
   "outputs": [],
   "source": [
    "kmeans_clustering_model=KMeans(n_clusters=10, random_state=0)\n",
    "kmeans_clustering_model.fit(clusterable_embedding)\n",
    "kmeans_label=kmeans_clustering_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EgtWDM2uDaKp"
   },
   "outputs": [],
   "source": [
    "df['cluster_']=kmeans_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wKyc19AdDsD5"
   },
   "source": [
    "Now we can compare to ways of clustering. Usually K-Means works well for “round” or spherical, and when most dense in the center of the sphere\n",
    "not contaminated by noise/outliers. Our dataset does not centered with arbitrary shapes and too many noises, therefore K-Means works well here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fCsyWHN-Jxk7"
   },
   "source": [
    "### visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rl3GR5mLZVpe"
   },
   "outputs": [],
   "source": [
    "plt.scatter(clusterable_embedding[:,0],clusterable_embedding[:,1], c=kmeans, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h8-IEzEiNzF_"
   },
   "source": [
    "####Body Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4EzKcgDpN17H"
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "distortions2 = []\n",
    "K2 = range(2, 40)\n",
    "for k in K:\n",
    "    k_means2 = KMeans(n_clusters=k, random_state=42).fit(clusterable_embedding2)\n",
    "    k_means2.fit(clusterable_embedding2)\n",
    "    distortions2.append(sum(np.min(cdist(clusterable_embedding2, k_means2.cluster_centers_, 'euclidean'), axis=1)) / pd.DataFrame(data3).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qkgELatAN_Xl"
   },
   "outputs": [],
   "source": [
    "X_line2 = [K2[0], K2[-1]]\n",
    "Y_line2 = [distortions2[0], distortions2[-1]]\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(K2, distortions2, 'b-')\n",
    "plt.plot(X_line2, Y_line2, 'r')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I7A2Hu1dOMPs"
   },
   "outputs": [],
   "source": [
    "kmeans2 = KMeans(n_clusters=10, random_state=0).fit_predict(clusterable_embedding2)\n",
    "kmeans2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fn-V_UUuORa_"
   },
   "outputs": [],
   "source": [
    "kmeans_clustering_model2=KMeans(n_clusters=10, random_state=0)\n",
    "kmeans_clustering_model2.fit(clusterable_embedding2)\n",
    "kmeans_label2=kmeans_clustering_model2.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VpyG_4UAOW-7"
   },
   "outputs": [],
   "source": [
    "df['textcluster_']=kmeans_label2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hCYm8P2WJxk8"
   },
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fjUlPQp-aHHk"
   },
   "source": [
    "After find the best clustering, we are going to labeled each topic. Most freequent methods of topic modeling, including latent Dirichlet Allocation.<sup>[9](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)</sup> , a generative statistical model that uses unobserved groups to explain why some parts of a corpus are similar, and Non-negative matrix factorization, does not woek for emedding method we choose. As for further work, we will re-preprocessing text, using TFIDFVectorizer or CountVectorizer to find the topic each models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XycuN3GUJxk_"
   },
   "source": [
    "# Semantic search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eZ1A7CvOJxlA"
   },
   "source": [
    "### Cosine similarity\n",
    "An amazing use of the embeddings we obtained with our fine-tuned Sentence Transformer model is to query the corpus and find the most similar embeddings to the query's embeddings. This can be simply done by calculating cosine similarity among embeddings, and then select the least-distant ones as the most relevant to the query.  \n",
    "Here we propose a semantic search system where the user can input a question about COVID-19 and they will get the top 5 most relevant articles from the corpus. We suggest to interrogate our system with questions taken from the Kaggle's suggested [tasks](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge/tasks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zsLj8wEdJxlD"
   },
   "outputs": [],
   "source": [
    "## call searches and get table out\n",
    "import searches\n",
    "\n",
    "# import model\n",
    "# embedder = SentenceTransformer('bert-large-nli-mean-tokens') #this or the model we trained and saved\n",
    "\n",
    "# load corpus\n",
    "df_covid = pd.read_pickle('complete_dataframe.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uQU0o492Aga1"
   },
   "outputs": [],
   "source": [
    "# asking the user\n",
    "query = input('What would you like to know from CORD-19? ')\n",
    "print('\\nUse abstracts:')\n",
    "searches.sem_search(query, embedder, df_covid, df_covid['abs_embeddings'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2StAq7D2Aga3"
   },
   "outputs": [],
   "source": [
    "# asking the user (slower)\n",
    "print('\\nUse full text:')\n",
    "searches.sem_search(query, embedder, df_covid, df_covid['body_embeddings'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-CUCxMIpJxlF"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TMOq2uhLJxlF"
   },
   "source": [
    "Great AI stuff I guess!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zb0XVjFxAga6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "CORD19_report.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
